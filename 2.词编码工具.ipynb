{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db43590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#在线加载一个tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a672bb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/google-bert/bert-base-chinese/tokenizer_config.json',\n",
       " 'tokenizer/google-bert/bert-base-chinese/special_tokens_map.json',\n",
       " 'tokenizer/google-bert/bert-base-chinese/vocab.txt',\n",
       " 'tokenizer/google-bert/bert-base-chinese/added_tokens.json',\n",
       " 'tokenizer/google-bert/bert-base-chinese/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#保存到本地\n",
    "tokenizer.save_pretrained('tokenizer/google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e0868c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='tokenizer/google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#从本地文件加载\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'tokenizer/google-bert/bert-base-chinese')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f883ea76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 1962, 117, 872, 1962, 1408, 136, 102]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#简单编码\n",
    "data = tokenizer.encode('你好,你好吗?')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f068d6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 你 好, 你 好 吗? [SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#解码\n",
    "tokenizer.decode(data, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676ed94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 872, 1962, 117, 872, 1962, 1408, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#常规编码\n",
    "data = tokenizer('你好,你好吗?')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "429770be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[ 101, 5018,  671,  702, 1368, 2094,  102, 5018,  676,  702, 1368,\n",
       "        2094,  102,    0,    0,    0,    0,    0],\n",
       "       [ 101, 5018,  753,  702, 1368, 2094,  102, 5018, 1724,  702, 3291,\n",
       "        7270,  671, 4157, 4638, 1368, 2094,  102]]), 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#参数解释\n",
    "data = tokenizer(\n",
    "\n",
    "    #句子的前半部分\n",
    "    text=['第一个句子', '第二个句子'],\n",
    "\n",
    "    #句子的后半部分,单句子编码时不用传递\n",
    "    text_pair=['第三个句子', '第四个更长一点的句子'],\n",
    "\n",
    "    #是否要添加特殊符号\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    #是否补长到统一长度,一般定义为True或者'max_length'\n",
    "    #定义为True时,编码的长度取决于最长的句子\n",
    "    #定义为max_length时,编码的长度就等于max_length\n",
    "    padding=True,\n",
    "\n",
    "    #句子长度超过max_length时是否裁剪,一般定义为True\n",
    "    truncation=True,\n",
    "\n",
    "    #定义最大编码长度\n",
    "    max_length=20,\n",
    "\n",
    "    #编码数据的格式,一般定义为'pt','np','tf'默认是list\n",
    "    return_tensors='np',\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c70bac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 第 一 个 句 子 [SEP] 第 三 个 句 子 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] 第 二 个 句 子 [SEP] 第 四 个 更 长 一 点 的 句 子 [SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#批量解码\n",
    "tokenizer.batch_decode(data.input_ids, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f9424e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 5018,  671,  702, 1368, 2094,  102, 5018,  676,  702, 1368,\n",
       "        2094,  102,    0,    0,    0,    0,    0],\n",
       "       [ 101, 5018,  753,  702, 1368, 2094,  102, 5018, 1724,  702, 3291,\n",
       "        7270,  671, 4157, 4638, 1368, 2094,  102]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#编码结果\n",
    "data.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c15bed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#标记每个句子中前后两段的位置,第二句的位置是1,其他是0\n",
    "data.token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de2c47a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#标记哪些位置是pad\n",
    "data.attention_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
