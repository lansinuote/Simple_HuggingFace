{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba6c5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f0e1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text'],\n",
       "         num_rows: 8130\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['text'],\n",
       "         num_rows: 1032\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text'],\n",
       "         num_rows: 1011\n",
       "     })\n",
       " }),\n",
       " {'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#加载数据集\n",
    "dataset = load_dataset(path='lansinuote/ChnSentiCorp')\n",
    "\n",
    "#过滤句子长度\n",
    "f = lambda x: len(x['text']) >= 40\n",
    "dataset = dataset.filter(f)\n",
    "\n",
    "#移除多余的字段\n",
    "dataset = dataset.remove_columns(['label'])\n",
    "\n",
    "dataset, dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213f86ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([4, 41])\n",
      "token_type_ids torch.Size([4, 41])\n",
      "attention_mask torch.Size([4, 41])\n",
      "label torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2032"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义数据集遍历工具\n",
    "def collate_fn(data):\n",
    "    b = len(data)\n",
    "    text = [i['text'] for i in data]\n",
    "\n",
    "    #生成前后两段话分别的索引\n",
    "    s1 = list(range(b))\n",
    "    s2 = list(range(b))\n",
    "    random.shuffle(s2)\n",
    "\n",
    "    #根据索引生成label,表明两句话是否是前后相连的关系\n",
    "    label = [s1[i] == s2[i] for i in range(b)]\n",
    "\n",
    "    #取出具体的文字\n",
    "    s1 = [text[i][0:20] for i in s1]\n",
    "    s2 = [text[i][20:40] for i in s2]\n",
    "\n",
    "    #句子对编码\n",
    "    data = tokenizer(s1,\n",
    "                     s2,\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     max_length=50,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "    #设置label\n",
    "    data['label'] = torch.LongTensor(label)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset['train'],\n",
    "                                     batch_size=4,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True,\n",
    "                                     collate_fn=collate_fn)\n",
    "\n",
    "data = next(iter(loader))\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262fd6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 1 ， 外 壳 晶 莹 剔 透 ， 注 重 外 形 的 人 可 以 选 择 ； [SEP] 0 - 100 的 东 券 ， 节 省 了 100 元 ， 哈 哈 。 [SEP] [PAD] [PAD]\n",
      "tensor(0)\n",
      "================\n",
      "[CLS] 东 西 很 超 值 ， 买 了 几 件 东 西 ， 用 了 一 个 200 [SEP] 都 很 低 的 时 候 ， 鼠 标 显 示 等 待 ， 就 像 是 死 机 了 [SEP]\n",
      "tensor(0)\n",
      "================\n",
      "[CLS] 唯 独 不 能 理 解 的 地 方 时 ， cpu 和 内 存 占 用 率 [SEP] 会 觉 得 自 己 过 得 兔 子 一 般 的 生 活 ， 当 然 ， 如 果 [SEP]\n",
      "tensor(0)\n",
      "================\n",
      "[CLS] 我 是 一 个 无 肉 不 欢 的 人 ， 如 果 每 天 不 吃 点 肉 就 [SEP] 2 ， 显 卡 属 于 中 端 卡 ， 一 般 [UNK] 游 戏 基 本 没 [SEP]\n",
      "tensor(0)\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "#查看数据样例\n",
    "for input_ids, label in zip(data['input_ids'], data['label']):\n",
    "    print(tokenizer.decode(input_ids))\n",
    "    print(label)\n",
    "    print('================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca64aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5830, grad_fn=<NllLossBackward0>),\n",
       " tensor([[0.6230, 0.3770],\n",
       "         [0.6774, 0.3226],\n",
       "         [0.4611, 0.5389],\n",
       "         [0.7289, 0.2711]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义模型\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #加载预训练模型\n",
    "        from transformers import AutoModel\n",
    "        self.pretrained = AutoModel.from_pretrained(\n",
    "            'google-bert/bert-base-chinese')\n",
    "\n",
    "        self.fc = torch.nn.Linear(in_features=768, out_features=2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, label=None):\n",
    "        #使用预训练模型抽取数据特征\n",
    "        with torch.no_grad():\n",
    "            last_hidden_state = self.pretrained(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids).last_hidden_state\n",
    "\n",
    "        #只取第0个词的特征做分类,这和bert模型的训练方式有关,此处不展开\n",
    "        last_hidden_state = last_hidden_state[:, 0]\n",
    "\n",
    "        #对抽取的特征只取第一个字的结果做分类即可\n",
    "        out = self.fc(last_hidden_state).softmax(dim=1)\n",
    "\n",
    "        #计算loss\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(out, label)\n",
    "\n",
    "        return loss, out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359353fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2032 0.5397893786430359 1.0\n",
      "10 2032 0.4697575569152832 1.0\n",
      "20 2032 0.5538473129272461 0.75\n",
      "30 2032 0.42610231041908264 1.0\n",
      "40 2032 0.6893507242202759 0.5\n",
      "50 2032 0.4129978120326996 1.0\n",
      "60 2032 0.4780278205871582 0.75\n",
      "70 2032 0.5568191409111023 0.75\n",
      "80 2032 0.47240906953811646 0.75\n",
      "90 2032 0.38953810930252075 1.0\n",
      "100 2032 0.5210029482841492 1.0\n",
      "110 2032 0.4725152850151062 1.0\n",
      "120 2032 0.4108600914478302 1.0\n",
      "130 2032 0.32916051149368286 1.0\n",
      "140 2032 0.5040423274040222 0.75\n",
      "150 2032 0.4608982801437378 0.75\n",
      "160 2032 0.35494521260261536 1.0\n",
      "170 2032 0.5046869516372681 0.75\n",
      "180 2032 0.42960476875305176 1.0\n",
      "190 2032 0.562688946723938 0.75\n",
      "200 2032 0.3796224594116211 1.0\n",
      "210 2032 0.6669230461120605 0.5\n",
      "220 2032 0.6008779406547546 0.75\n",
      "230 2032 0.5519516468048096 0.5\n",
      "240 2032 0.7723520398139954 0.25\n",
      "250 2032 0.43430188298225403 1.0\n",
      "260 2032 0.37184998393058777 1.0\n",
      "270 2032 0.40778613090515137 1.0\n",
      "280 2032 0.32381296157836914 1.0\n",
      "290 2032 0.33126550912857056 1.0\n",
      "300 2032 0.4859747290611267 0.75\n"
     ]
    }
   ],
   "source": [
    "#执行训练\n",
    "def train():\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "        loss, out = model(**data)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            acc = (out == data.label).sum().item() / len(data.label)\n",
    "            print(i, len(loader), loss.item(), acc)\n",
    "\n",
    "        if i == 300:\n",
    "            break\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "628f7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 252 0.25\n",
      "1 252 0.625\n",
      "2 252 0.75\n",
      "3 252 0.8125\n",
      "4 252 0.85\n",
      "5 252 0.875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#执行测试\n",
    "def test():\n",
    "    loader_test = torch.utils.data.DataLoader(dataset['test'],\n",
    "                                              batch_size=4,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True,\n",
    "                                              collate_fn=collate_fn)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(loader_test):\n",
    "        with torch.no_grad():\n",
    "            _, out = model(**data)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == data.label).sum().item()\n",
    "        total += len(data.label)\n",
    "\n",
    "        print(i, len(loader_test), correct / total)\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
