{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba6c5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f0e1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 9600\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 1200\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 1200\n",
       "     })\n",
       " }),\n",
       " {'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "  'label': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#加载数据集\n",
    "dataset = load_dataset(path='lansinuote/ChnSentiCorp')\n",
    "\n",
    "dataset, dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213f86ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 185])\n",
      "attention_mask torch.Size([8, 185])\n",
      "label torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义数据集遍历工具\n",
    "def collate_fn(data):\n",
    "    text = [i['text'] for i in data]\n",
    "    label = [i['label'] for i in data]\n",
    "\n",
    "    #文字编码\n",
    "    data = tokenizer(text,\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     max_length=500,\n",
    "                     return_tensors='pt',\n",
    "                     return_token_type_ids=False)\n",
    "\n",
    "    #设置label\n",
    "    data['label'] = torch.LongTensor(label)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset['train'],\n",
    "                                     batch_size=8,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True,\n",
    "                                     collate_fn=collate_fn)\n",
    "\n",
    "data = next(iter(loader))\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec109537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6372, grad_fn=<NllLossBackward0>),\n",
       " tensor([[0.7080, 0.2920],\n",
       "         [0.6366, 0.3634],\n",
       "         [0.5499, 0.4501],\n",
       "         [0.5178, 0.4822],\n",
       "         [0.5974, 0.4026],\n",
       "         [0.6423, 0.3577],\n",
       "         [0.5603, 0.4397],\n",
       "         [0.5908, 0.4092]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义模型\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #加载预训练模型\n",
    "        from transformers import AutoModel\n",
    "        self.pretrained = AutoModel.from_pretrained(\n",
    "            'google-bert/bert-base-chinese')\n",
    "\n",
    "        self.fc = torch.nn.Linear(in_features=768, out_features=2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label=None):\n",
    "        #使用预训练模型抽取数据特征\n",
    "        with torch.no_grad():\n",
    "            last_hidden_state = self.pretrained(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        #只取第0个词的特征做分类,这和bert模型的训练方式有关,此处不展开\n",
    "        last_hidden_state = last_hidden_state[:, 0]\n",
    "\n",
    "        #对抽取的特征只取第一个字的结果做分类即可\n",
    "        out = self.fc(last_hidden_state).softmax(dim=1)\n",
    "\n",
    "        #计算loss\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(out, label)\n",
    "\n",
    "        return loss, out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762606f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1200 0.6553741693496704 0.75\n",
      "10 1200 0.6900417804718018 0.5\n",
      "20 1200 0.6871982216835022 0.375\n",
      "30 1200 0.7322638630867004 0.375\n",
      "40 1200 0.6802827715873718 0.625\n",
      "50 1200 0.6234102249145508 0.75\n",
      "60 1200 0.665240466594696 0.75\n",
      "70 1200 0.6254176497459412 0.875\n",
      "80 1200 0.6244698762893677 0.875\n",
      "90 1200 0.6492530107498169 0.875\n",
      "100 1200 0.6473492383956909 0.5\n",
      "110 1200 0.7369059324264526 0.5\n",
      "120 1200 0.5942272543907166 0.75\n",
      "130 1200 0.612605094909668 0.875\n",
      "140 1200 0.5870425701141357 1.0\n",
      "150 1200 0.5847880840301514 0.75\n",
      "160 1200 0.5123168230056763 1.0\n",
      "170 1200 0.6124182939529419 0.625\n",
      "180 1200 0.5733353495597839 1.0\n",
      "190 1200 0.5715835094451904 0.75\n",
      "200 1200 0.5075149536132812 1.0\n",
      "210 1200 0.5954742431640625 0.875\n",
      "220 1200 0.7148230075836182 0.5\n",
      "230 1200 0.4510529339313507 1.0\n",
      "240 1200 0.6033474802970886 0.75\n",
      "250 1200 0.5930602550506592 0.625\n",
      "260 1200 0.511476993560791 1.0\n",
      "270 1200 0.4931146800518036 1.0\n",
      "280 1200 0.520031750202179 0.75\n",
      "290 1200 0.550284206867218 0.875\n",
      "300 1200 0.5650743246078491 0.875\n"
     ]
    }
   ],
   "source": [
    "#执行训练\n",
    "def train():\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "        loss, out = model(**data)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            acc = (out == data.label).sum().item() / len(data.label)\n",
    "            print(i, len(loader), loss.item(), acc)\n",
    "\n",
    "        if i == 300:\n",
    "            break\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a589baf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 150 1.0\n",
      "1 150 0.9375\n",
      "2 150 0.875\n",
      "3 150 0.90625\n",
      "4 150 0.875\n",
      "5 150 0.875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#执行测试\n",
    "def test():\n",
    "    loader_test = torch.utils.data.DataLoader(dataset['test'],\n",
    "                                              batch_size=8,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True,\n",
    "                                              collate_fn=collate_fn)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(loader_test):\n",
    "        with torch.no_grad():\n",
    "            _, out = model(**data)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == data.label).sum().item()\n",
    "        total += len(data.label)\n",
    "\n",
    "        print(i, len(loader_test), correct / total)\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
