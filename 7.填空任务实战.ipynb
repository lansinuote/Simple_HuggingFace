{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba6c5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926719c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0bd63912784778a91e4e614b443e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea16d436e1ec42219422b0d154522fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b345cf11538d423e87cec69a324529fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'label'],\n",
       "         num_rows: 9286\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'label'],\n",
       "         num_rows: 1158\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'label'],\n",
       "         num_rows: 1157\n",
       "     })\n",
       " }),\n",
       " {'input_ids': tensor([ 101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221,\n",
       "          3175,  912, 8024,  103, 4510, 1220, 2820, 3461, 4684, 2970, 1168, 6809,\n",
       "          3862, 6804, 8024, 1453, 1741,  102]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1]),\n",
       "  'label': tensor(3300)})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#加载数据集\n",
    "dataset = load_dataset(path='lansinuote/ChnSentiCorp')\n",
    "\n",
    "#编码\n",
    "f = lambda x: tokenizer(\n",
    "    x['text'], truncation=True, max_length=30, return_token_type_ids=False)\n",
    "dataset = dataset.map(f, remove_columns=['text', 'label'])\n",
    "\n",
    "#过滤句子长度\n",
    "f = lambda x: len(x['input_ids']) >= 30\n",
    "dataset = dataset.filter(f)\n",
    "\n",
    "\n",
    "#重置label字段\n",
    "def f(data):\n",
    "    #定义第15个字为label\n",
    "    data['label'] = data['input_ids'][15]\n",
    "\n",
    "    #替换句子中的第15个字为mask\n",
    "    data['input_ids'][15] = tokenizer.mask_token_id\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "dataset = dataset.map(f)\n",
    "\n",
    "#设置数据类型\n",
    "dataset.set_format('pt')\n",
    "\n",
    "dataset, dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213f86ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 30])\n",
      "attention_mask torch.Size([8, 30])\n",
      "label torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1160"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset['train'],\n",
    "                                     batch_size=8,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "data = next(iter(loader))\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eacef06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 今 年 的 5. 1 、 10. 1 长 假 分 别 [MASK] 了 西 安 与 重 庆 的 如 家 ， 非 常 失 [SEP]\n",
      "定\n",
      "==============\n",
      "[CLS] 觉 得 很 多 书 都 比 这 书 好 ， 看 到 排 [MASK] 榜 的 排 名 才 买 它 的 ， 完 全 没 内 [SEP]\n",
      "行\n",
      "==============\n",
      "[CLS] 在 电 视 上 见 到 于 丹 ， 佩 服 她 的 口 [MASK] 和 思 维 ， 就 买 了 这 本 书 ， 但 有 [SEP]\n",
      "才\n",
      "==============\n",
      "[CLS] 内 存 小 ， 才 [UNK] ， 不 是 独 立 显 卡 ， [MASK] 摄 像 头 ， 硬 盘 不 是 [UNK] 。 接 口 布 [SEP]\n",
      "无\n",
      "==============\n",
      "[CLS] 真 正 说 起 来 ， 缺 点 还 是 不 少 的 。 [MASK] 先 ， 机 器 比 较 重 ， 价 格 不 便 宜 [SEP]\n",
      "首\n",
      "==============\n",
      "[CLS] 可 惜 机 器 wifi 不 支 持 wpa2 - psk, [MASK] 后 用 限 定 [UNK] 不 设 密 码 连 接 。 机 [SEP]\n",
      "最\n",
      "==============\n",
      "[CLS] 自 己 的 体 质 是 什 么 类 型 的 ， 该 怎 [MASK] 养 生 ， 看 了 这 本 书 让 我 一 下 子 [SEP]\n",
      "样\n",
      "==============\n",
      "[CLS] 性 价 比 不 用 说 了 ， 4455 买 的 ， 配 [MASK] 也 比 较 正 常 些 ， 工 作 娱 乐 看 电 [SEP]\n",
      "置\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "#查看数据样例\n",
    "for q, a in zip(data['input_ids'], data['label']):\n",
    "    print(tokenizer.decode(q))\n",
    "    print(tokenizer.decode(a))\n",
    "    print('==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca64aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.9583, grad_fn=<NllLossBackward0>),\n",
       " tensor([[6.2859e-05, 2.4186e-05, 3.1236e-05,  ..., 7.5827e-05, 5.1049e-05,\n",
       "          5.9941e-05],\n",
       "         [9.8141e-05, 2.9037e-05, 3.0801e-05,  ..., 6.9947e-05, 3.2658e-05,\n",
       "          3.3411e-05],\n",
       "         [6.8259e-05, 2.3807e-05, 3.0539e-05,  ..., 3.6393e-05, 4.8572e-05,\n",
       "          4.5045e-05],\n",
       "         ...,\n",
       "         [7.5431e-05, 2.5728e-05, 3.0282e-05,  ..., 7.1031e-05, 8.0782e-05,\n",
       "          3.5735e-05],\n",
       "         [6.8091e-05, 1.9636e-05, 3.0080e-05,  ..., 1.0155e-04, 3.7200e-05,\n",
       "          5.9929e-05],\n",
       "         [5.9883e-05, 2.7097e-05, 4.7808e-05,  ..., 1.0384e-04, 8.2132e-05,\n",
       "          3.7258e-05]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义模型\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #加载预训练模型\n",
    "        from transformers import AutoModel\n",
    "        self.pretrained = AutoModel.from_pretrained(\n",
    "            'google-bert/bert-base-chinese')\n",
    "\n",
    "        self.fc = torch.nn.Linear(in_features=768,\n",
    "                                  out_features=tokenizer.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label=None):\n",
    "        #使用预训练模型抽取数据特征\n",
    "        with torch.no_grad():\n",
    "            last_hidden_state = self.pretrained(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        #取第15个词的特征向量\n",
    "        last_hidden_state = last_hidden_state[:, 15]\n",
    "\n",
    "        #对抽取的特征只取第一个字的结果做分类即可\n",
    "        out = self.fc(last_hidden_state).softmax(dim=1)\n",
    "\n",
    "        #计算loss\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(out, label)\n",
    "\n",
    "        return loss, out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359353fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1160 9.958341598510742 0.0\n",
      "0 200 1160 9.95827865600586 0.0\n",
      "0 400 1160 9.956317901611328 0.0\n",
      "0 600 1160 9.840551376342773 0.25\n",
      "0 800 1160 9.957660675048828 0.0\n",
      "0 1000 1160 9.902223587036133 0.125\n",
      "1 0 1160 9.770970344543457 0.25\n",
      "1 200 1160 9.732255935668945 0.375\n",
      "1 400 1160 9.722811698913574 0.25\n",
      "1 600 1160 9.78173542022705 0.25\n",
      "1 800 1160 9.731805801391602 0.25\n",
      "1 1000 1160 9.618192672729492 0.375\n",
      "2 0 1160 9.832657814025879 0.125\n",
      "2 200 1160 9.787606239318848 0.25\n",
      "2 400 1160 9.686799049377441 0.375\n",
      "2 600 1160 9.558889389038086 0.5\n",
      "2 800 1160 9.681939125061035 0.25\n",
      "2 1000 1160 9.501626968383789 0.5\n",
      "3 0 1160 9.32802677154541 0.75\n",
      "3 200 1160 9.61976146697998 0.5\n",
      "3 400 1160 9.431528091430664 0.625\n",
      "3 600 1160 9.483133316040039 0.625\n",
      "3 800 1160 9.233800888061523 0.875\n",
      "3 1000 1160 9.9302396774292 0.125\n",
      "4 0 1160 9.478179931640625 0.5\n",
      "4 200 1160 9.35655403137207 0.75\n",
      "4 400 1160 9.726234436035156 0.375\n",
      "4 600 1160 9.468263626098633 0.5\n",
      "4 800 1160 9.402331352233887 0.625\n",
      "4 1000 1160 9.340301513671875 0.625\n"
     ]
    }
   ],
   "source": [
    "#执行训练\n",
    "def train():\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        for i, data in enumerate(loader):\n",
    "            loss, out = model(**data)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                out = out.argmax(dim=1)\n",
    "                acc = (out == data['label']).sum().item() / len(data['label'])\n",
    "                print(epoch, i, len(loader), loss.item(), acc)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "628f7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 144 0.125\n",
      "1 144 0.375\n",
      "2 144 0.375\n",
      "3 144 0.40625\n",
      "4 144 0.475\n",
      "5 144 0.4791666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4791666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#执行测试\n",
    "def test():\n",
    "    loader_test = torch.utils.data.DataLoader(dataset['test'],\n",
    "                                              batch_size=8,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(loader_test):\n",
    "        with torch.no_grad():\n",
    "            _, out = model(**data)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == data['label']).sum().item()\n",
    "        total += len(data['label'])\n",
    "\n",
    "        print(i, len(loader_test), correct / total)\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
